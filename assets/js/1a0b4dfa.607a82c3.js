"use strict";(self.webpackChunkslop_site=self.webpackChunkslop_site||[]).push([[4877],{3905:(e,t,r)=>{r.d(t,{Zo:()=>l,kt:()=>m});var n=r(7294);function o(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function a(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function s(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?a(Object(r),!0).forEach((function(t){o(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):a(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function i(e,t){if(null==e)return{};var r,n,o=function(e,t){if(null==e)return{};var r,n,o={},a=Object.keys(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||(o[r]=e[r]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(o[r]=e[r])}return o}var u=n.createContext({}),c=function(e){var t=n.useContext(u),r=t;return e&&(r="function"==typeof e?e(t):s(s({},t),e)),r},l=function(e){var t=c(e.components);return n.createElement(u.Provider,{value:t},e.children)},p="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var r=e.components,o=e.mdxType,a=e.originalType,u=e.parentName,l=i(e,["components","mdxType","originalType","parentName"]),p=c(r),d=o,m=p["".concat(u,".").concat(d)]||p[d]||h[d]||a;return r?n.createElement(m,s(s({ref:t},l),{},{components:r})):n.createElement(m,s({ref:t},l))}));function m(e,t){var r=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=r.length,s=new Array(a);s[0]=d;var i={};for(var u in t)hasOwnProperty.call(t,u)&&(i[u]=t[u]);i.originalType=e,i[p]="string"==typeof e?e:o,s[1]=i;for(var c=2;c<a;c++)s[c]=r[c];return n.createElement.apply(null,s)}return n.createElement.apply(null,r)}d.displayName="MDXCreateElement"},3710:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>u,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>c});var n=r(7462),o=(r(7294),r(3905));const a={sidebar_position:1},s="Saving / Loading Lexer Output",i={unversionedId:"Language/Extra Features/save-load-lexer-output",id:"Language/Extra Features/save-load-lexer-output",title:"Saving / Loading Lexer Output",description:"As discussed in the Overview section, there are two parts to the execution process which involve the",source:"@site/docs/Language/Extra Features/save-load-lexer-output.md",sourceDirName:"Language/Extra Features",slug:"/Language/Extra Features/save-load-lexer-output",permalink:"/slop-site/docs/Language/Extra Features/save-load-lexer-output",draft:!1,editUrl:"https://gitlab.com/tronied/slop/docs/Language/Extra Features/save-load-lexer-output.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Variables",permalink:"/slop-site/docs/Language/Statements/variables"},next:{title:"Chained Expressions",permalink:"/slop-site/docs/Language/Extra Features/chained-expressions"}},u={},c=[],l={toc:c};function p(e){let{components:t,...r}=e;return(0,o.kt)("wrapper",(0,n.Z)({},l,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"saving--loading-lexer-output"},"Saving / Loading Lexer Output"),(0,o.kt)("p",null,"As discussed in the ",(0,o.kt)("a",{parentName:"p",href:"#overview"},"Overview")," section, there are two parts to the execution process which involve the\nLexer and Parser. The Lexer transforms an expression into a series of tokens, whereas the Parser uses those tokens\nand any referenced objects to calculate a result. Although it may seem like the calculation would be the most costly\npart, the opposite is in fact true. Lexing code or expressions is generally much more expensive due to the logic\nrequired to keep track of which token is being written to and where. This is why compilers are faster than\ninterpreters as they are able to transform (compile) written code into machine code which can be stored to disk and\nvery fast to execute. Using that same logic, SLOP can serialize and persist the tokens to file so that those expressions\ncan be loaded once and executed in token form in a fraction of the normal time. This can be useful when you may want\nto run different values through the same expression without having to lex the expression each time."),(0,o.kt)("p",null,"To do this there are two methods called tokenizeToString and tokenizeToFile on the SLOPProcessor object. Each can\nbe used for different purposes but both accept an expression and persist the Lexer output to String or File."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-java"},'SLOPProcessor processor = new SLOPProcessor();\nString serializedOutput = processor.processToString("[1,2 * 1,SUM(1,2,3) + 1]");\n')),(0,o.kt)("p",null,"Likewise for reading serialized output either in String or File form there are two methods (processFromString /\nprocessFromFile) in the SLOPProcessor();"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-java"},"ExpressionResult<?> result = processor.processFromString(serializedOutput);\n")),(0,o.kt)("p",null,"To provide one example where these features could be used, imagine you have a document generator which contains\none or more templates containing expressions. If you have 1000 customers all of which need statements you could\nevaluate each expression in the document each time but this would be very slow. What we could do with the above\nfunctionality is to serialize the expressions within the template to file and reference each using a unique key.\nWhen the system / service starts up these are loaded into memory (a Map object for example) so that as the document\nis being processed instead of evaluating each expression the tokens can be run through the Parser with the relevant\nobjects to resolve the result. This would result in the 1000 documents being created orders of magnitude faster\nthan the former method."))}p.isMDXComponent=!0}}]);