"use strict";(self.webpackChunkslop_site=self.webpackChunkslop_site||[]).push([[4328],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>u});var o=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=o.createContext({}),h=function(e){var t=o.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=h(e.components);return o.createElement(l.Provider,{value:t},e.children)},k="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},c=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),k=h(n),c=a,u=k["".concat(l,".").concat(c)]||k[c]||d[c]||r;return n?o.createElement(u,i(i({ref:t},p),{},{components:n})):o.createElement(u,i({ref:t},p))}));function u(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,i=new Array(r);i[0]=c;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[k]="string"==typeof e?e:a,i[1]=s;for(var h=2;h<r;h++)i[h]=n[h];return o.createElement.apply(null,i)}return o.createElement.apply(null,n)}c.displayName="MDXCreateElement"},15:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>k,frontMatter:()=>r,metadata:()=>s,toc:()=>h});var o=n(7462),a=(n(7294),n(3905));const r={sidebar_position:6},i="Pattern Tokens",s={unversionedId:"Extending/pattern-tokens",id:"Extending/pattern-tokens",title:"Pattern Tokens",description:"Although this section is not strictly necessary for extending SLOP, it describes the mechanism through which more",source:"@site/docs/Extending/pattern-tokens.md",sourceDirName:"Extending",slug:"/Extending/pattern-tokens",permalink:"/docs/Extending/pattern-tokens",draft:!1,editUrl:"https://gitlab.com/tronied/slop/docs/Extending/pattern-tokens.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{sidebar_position:6},sidebar:"tutorialSidebar",previous:{title:"Adding Statements",permalink:"/docs/Extending/adding-statements"},next:{title:"Questions & Answers",permalink:"/docs/support"}},l={},h=[{value:"Additional Grammar / Pattern Flags",id:"additional-grammar--pattern-flags",level:2}],p={toc:h};function k(e){let{components:t,...n}=e;return(0,a.kt)("wrapper",(0,o.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"pattern-tokens"},"Pattern Tokens"),(0,a.kt)("p",null,"Although this section is not strictly necessary for extending SLOP, it describes the mechanism through which more\ncomplex tokens (statements) are identified and updated. As opposed to the standard literal tokens which use regular\nexpressions to match and have their values set, statements are added to a stack and appended to until the token is\nmarked as complete and added to the list of resulting tokens."),(0,a.kt)("p",null,"Without wanting to skip over how literals are scanned, a loop scans through the expression String and each token's\npattern is then evaluated against it to see if there is a match. Take the following:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"12345.99 + 100 / 50\n")),(0,a.kt)("p",null,"Using the following token patterns:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"DecimalToken = ^-?[0-9]+\\.[0-9]+\nIntegerToken = ^-?[0-9]+\nOperatorToken = ^(\\+\\+|--|\\+|-|\\/|\\*|>=|<=|<|>|%|!=|==)\n")),(0,a.kt)("p",null,"We can perform a loop now on the expression, removing each match until it is empty. The String is trimmed after each\niteration which matches other languages where whitespace is ignored. Using this we see the following iteration events:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"1) DecimalToken matched with value '12345.99'\n2) OperatorToken matched with the value '+'\n3) IntegerToken matched with the value '100'\n4) OperatorToken matched with value '/'\n5) IntegerToken matched with value '50'\n")),(0,a.kt)("p",null,"This is a very simple example, but the same basic premise is used for statements. The only difference here is that the\ngrammar pattern is mapped into a series of hierarchical pattern tokens. This is used to keep track of the location of\nthe last read token and what we are expecting the next token to be. For example, using the OperatorToken as an example\nwhich has the following pattern:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"'(' expr ')'\n")),(0,a.kt)("p",null,"This is mapped to the following pattern:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"GrammarGroup(position = 0, tokens = [\n    GrammarValue('('),\n    GrammarExpression(),\n    GrammarValue(')')\n])\n")),(0,a.kt)("p",null,"All pattern tokens (even if they are not part of a defined group in the pattern) will be part of a GrammarGroup. This\nis because the GrammarGroup is used to record and track the current position within the token. If we assume that we\nonly have this statement in the list of token handlers, with the following expression:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"1 + (2 + 3)\n")),(0,a.kt)("p",null,"We first read the Integer and Operator tokens and add them to the resulting token list. However, when we get to\nthe '(', none of the literals trigger a match. A check of the topmost stack item is made first to ensure that it is\nnot the next logical token in that. As our stack is empty however, we look for a new match in the token handlers\nlist and find a match with the OperatorToken. A new instance of that token is created and added to the top of the stack\nand the parent GrammarGroup pattern token's position is updated to 1. The position always points to the next expected\ntoken that the token is expecting. In this case the positions are 0-indexed hence 1 pointing to the GrammarExpression."),(0,a.kt)("p",null,"The Lexer then reads three more tokens which are an IntegerToken, OperatorToken and a final IntegerToken. As the next\ntoken after the GrammarExpression is a closing bracket ')', all of these get added to the child token list of the\nOperatorToken. Finally the closing brace is read, the tokens position is updated and marked as complete and removed\nfrom the top of the stack and added to the resulting token list:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"[== Resulting Tokens ==]\nIntegerToken(1),\nOperatorToken(+),\nOperationToken(tokens = [\n    TokenGroup(tokens = [      \n        IntegerToken(2),\n        OperatorToken(+),\n        IntegerToken(3)\n    ])    \n])\n")),(0,a.kt)("p",null,"This is a very simple example, but what happens if we have embedded statements? This is where the stack comes more\ninto play. Let's take the following expression as an example:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'((1 - 2) * 3) / 4 < -1 ? "no" : "yes"\n')),(0,a.kt)("p",null,"In the above although it may look simple, we have 3 levels of statements being processed. For this example I will\ndescribe each stage and provide the state of the stack. In the beginning our stack is empty and our first read\ntoken is an open bracket. Like the last example we have a match in the OperationToken which gets created and added\nto the top of the stack. Unlike last time though, the next read token is also an open bracket. As our top-most\nstack item is not expecting a closing bracket without at least some content, we look to the token handler list\nand again match the OperationToken which again gets created and added to the stack."),(0,a.kt)("p",null,"After the first two tokens have been evaluated, this is the state of our stack:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"[== Stack ==]\nOperationToken(position = 1, tokens = []) <-- Stack Top\nOperationToken(position = 1, tokens = [])\n")),(0,a.kt)("p",null,"The next three tokens are read and as they are two literals and an operator they are added to the top-most stack item.\nThe next token is a closing bracket which matches the next logical token in our top-most stack item. As such, it's\nposition is increased to the next logical token. In this case though as there are only 3 pattern tokens that form the\nOperatorToken, the position actually exceeds the number of tokens. This triggers the token to be flagged as complete.\nThe current state of the stack matches the following:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"[== Stack ==]\nOperationToken(position = 3, tokens = [\n    TokenGroup(tokens = [      \n        IntegerToken(1),\n        OperatorToken(-),\n        IntegerToken(2)\n    ])\n])\nOperationToken(position = 1, tokens = [])\n")),(0,a.kt)("p",null,"Prior to the next token being read, a check is made to check the state of the current top-most stack item. In this case\nit is found to be complete and as such it is removed from the stack and because the stack is not empty added as a child\nof the next stack item:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"[== Stack ==]\nOperationToken(position = 1, tokens = [\n    TokenGroup(tokens = [      \n        OperationToken(position = 3, tokens = [\n            TokenGroup(tokens = [                  \n                IntegerToken(1),\n                OperatorToken(-),\n                IntegerToken(2)\n            ])\n        ])\n    ])    \n])\n")),(0,a.kt)("p",null,"Two more tokens are then read which are an OperatorToken(*) and an IntegerToken(4). These are added to the top-most stack\nitem and a final closing brace is recorded which causes the last remaining item on the stack to be marked as complete:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"[== Stack ==]\nOperationToken(position = 3, tokens = [\n    TokenGroup(tokens = [      \n        OperationToken(position = 3, tokens = [\n            TokenGroup(tokens = [                  \n                IntegerToken(1),\n                OperatorToken(-),\n                IntegerToken(2)\n            ]),            \n        ]),\n        OperatorToken(*),\n        IntegerToken(3)\n    ]),    \n])\n")),(0,a.kt)("p",null,"The OperationToken as the last remaining token is popped from the top of the stack and added to the resulting list of tokens.\nFour more tokens are read and added to the token list which are the OperatorToken(/), IntegerToken(4), OperatorToken(<) and\nfinally an IntegerToken(-1). Without moving into the job of the Parser, if the expression ended here this would result in\na condition which would result in a Boolean. As it is, the list of tokens looks like the following:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"[== Resulting Tokens ==]\nOperationToken(position = 3, tokens = [\n    TokenGroup(tokens = [  \n        OperationToken(position = 3, tokens = [\n            TokenGroup(tokens = [              \n                IntegerToken(1),\n                OperatorToken(-),\n                IntegerToken(2)\n            ]),            \n        ]),\n        OperatorToken(*),\n        IntegerToken(3)\n    ]),    \n]),\nOperatorToken(/),\nIntegerToken(4),\nOperatorToken(<),\nIntegerToken(-1)\n")),(0,a.kt)("p",null,"Something then unexpected happens where we find a '?' token. This is part of a token called a Conditional which has the\nfollowing grammar pattern:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"expr '?' expr ':' expr\n")),(0,a.kt)("p",null,"What is strange about this is that we had no prior warning that the previously lexed tokens were part of a statement. You\nmay also notice that the '?' is not the first token in the pattern. Since that was an expression capture group, the first which\ncan be matched is actually the second token which says something about the statement's nature. The conditional is greedy and\nscoops up tokens immediately before and after as it has no strict opening and closing syantax tags. The start and end of the\nstatement can be identified by looking at those which share the same level / scope as the statement itself. For this\nexample a match is made against the ConditionalToken and a new instance added onto the stack with all prior tokens added.\nThe new state of the stack looks like the following:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"[== Stack ==]\nConditionalToken(position = 2, tokens = [\n    TokenGroup(tokens = [    \n        OperationToken(position = 3, tokens = [\n            TokenGroup(tokens = [\n                OperationToken(position = 3, tokens = [\n                    IntegerToken(1),\n                    OperatorToken(-),\n                    IntegerToken(2)\n                ])\n            ]),\n            OperatorToken(*),\n            IntegerToken(3)\n        ]),\n        OperatorToken(/),\n        IntegerToken(4),\n        OperatorToken(<),\n        IntegerToken(-1)\n    ]),    \n])\n")),(0,a.kt)("p",null,"As the active token is now a different grammar expression, all captured tokens will be written to a new token group. As\nthere is only a single String token, that is added to that. The next token is a ':' which matches the next logical token\nof the head of the stack in the Conditional statement. The position is incremented again to the position past the last\ntoken so that it now points to position 4 which is the last 'expr' capture group. As the last remaining token is a\nsolitary StringToken as well, the final picture of the stack prior to the ConditionalToken being removed looks like the\nfollowing:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'[== Stack ==]\nConditionalToken(position = 4, tokens = [\n    TokenGroup(tokens = [    \n        OperationToken(position = 3, tokens = [\n            ...\n        ]),\n        ...\n        IntegerToken(-1)        \n    ]),\n    TokenGroup(tokens = [    \n        StringToken("no")\n    ]),\n    TokenGroup(tokens = [    \n        StringToken("yes")\n    ]),        \n])\n')),(0,a.kt)("p",null,"As the lexer has now reached the end of the expression and there are no further tokens to be added, a finalizeExpression\nmethod is called which removes any tokens still on the stack (in this case the ConditionalToken) and adds it to the\nresulting token list. These are then returned to the SLOPProcessor and can either be serialized to a String or File or\npassed to the Parser for evaluation and resolution."),(0,a.kt)("h2",{id:"additional-grammar--pattern-flags"},"Additional Grammar / Pattern Flags"),(0,a.kt)("p",null,"In the above section we have covered how pattern tokens are used by the Lexer to keep track of where in the statement\nread tokens belong. There are several grammar flags which cause this flow to be disrupted or change. The first of these\nis the '+' flag which can be applied to any Grammar Group in a String. For example, taking the FieldToken pattern we\ncan see this being used to represent a repeating pattern:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"( val ( '[' expr ']' )? '.'? )+\n")),(0,a.kt)("p",null,"The entire pattern is wrapped in a parent grammar group which can be repeated one or more times (denoted by the following\n'+' character). Inside is a 'val' representing a single value capture group with an optional following grammar group\n(denoted by the following '?') with a set of square brackets surrounding an expression capture group. Finally this is\nfollowed by an optional syntax period character ('.'). If we provide some examples it should become clear as to the\nvalues it is trying to capture:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'object.field\nobject.collection[0].field\nobject.map["value" + 1]\n')),(0,a.kt)("p",null,"As can be seen some of the examples have index / key references whilst others do not. This is where the '?' tokens allows\noptional values to be captured. Going further we can split the middle example up to the following:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"object.\ncollection[0].\nfield\n")),(0,a.kt)("p",null,"Each one represents an individual capture event of the repeated grammar group pattern defined above. Likewise, each one\nwill be captured in its own TokenGroup. Let's then start running over the above example and see how the pattern tokens\nare structured and see how the Lexer handles them. The grammar lexer will output the following pattern tokens onto the\nFieldToken when SLOP is initialised:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"GrammarGroup(position = 0, multiple = true, tokens = [\n    GrammarValue(capture = true),\n    GrammarGroup(position = 0, optional = true, tokens = [\n        GrammarValue('['),\n        GrammarExpression(),\n        GrammarValue(']'),\n    ]),\n    GrammarValue(value = '.', optional = true)\n])\n")),(0,a.kt)("p",null,"Reading the expression String 'object.collection","[0]",".field', the first read token is a TokenValue('object') and so far\nno match occurs is found in our pattern as it is too generic. Keep in mind that a match only happens if an identifying\ntag (typically syntax) is read. The next token however is a match for the '.' found following the optional grammar group.\nThis may seem a bit odd as we have skipped the entire second GrammarGroup, but keep in mind the Lexer will only apply\nstrict matching if it is required. As this is optional, it skipped ahead and looked for a matching token which it found\nin the optional GrammarValue('.'). Upon finding this match, a new instance of the FieldToken is added to the top of the\nstack and TokenValue('object') is added into a TokenGroup."),(0,a.kt)("p",null,"The position of the group is then set to the position after the '.' which in this case exceeds the number of pattern\ntokens in the group. A check is triggered and since the group supports multiple occurances, it's position and all\ncontained group positions are reset back to their initial positions. At this stage the state of the stack looks like\nthe following:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"[== Stack ==]\nFieldToken(tokens = [\n    TokenGroup(tokens = [    \n        TokenValue('object')  \n    ])  \n])\n")),(0,a.kt)("p",null,"The next token is another TokenValue('collection') which gets added to the short term memory list of tokens. This is used\nto store recently read tokens which we're not quite sure what to do with yet. The next token is a TokenValue('[') which\ntriggers a match in the FieldToken on the stack but for a different reason than the previous. On this occasion instead\nof skipping over the optional group it has found a match in the starting syntax token. Looking at the pattern tokens\nagain we can see how the positions are updated to reflect this:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"GrammarGroup(position = 1, multiple = true, tokens = [\n    GrammarValue(capture = true),\n    GrammarGroup(position = 1, optional = true, tokens = [\n        GrammarValue('['),\n        GrammarExpression(),\n        GrammarValue(']'),\n    ]),\n    GrammarValue(value = '.', optional = true)\n])\n")),(0,a.kt)("p",null,"The current active pattern token is a grammar expression and will be used to capture any tokens found within the square\nbrackets. In our case it is just a single IntegerToken(0) and after the subsequent closing TokenValue(']') is read, the\nstack looks like the following:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"[== Stack ==]\nFieldToken(tokens = [\n    TokenGroup(tokens = [\n        TokenGroup(tokens = [    \n            TokenValue('object')  \n        ]),\n        TokenGroup(tokens = [    \n            TokenValue('collection'),\n            TokenGroup(tokens = [\n                IntegerToken(0)\n            ])  \n        ])\n    ])        \n])\n")),(0,a.kt)("p",null,"The pattern tokens with their positions are the following:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"[== Pattern Tokens ==]\nGrammarGroup(position = 2, multiple = true, tokens = [\n    GrammarValue(capture = true),\n    GrammarGroup(position = 3, optional = true, tokens = [\n        GrammarValue('['),\n        GrammarExpression(),\n        GrammarValue(']'),\n    ]),\n    GrammarValue(value = '.', optional = true)\n])\n")),(0,a.kt)("p",null,"The current active token is pointing to the GrammarValue('.') which is the next token read by the Lexer. Again the\nposition is updated and the group is marked as complete, a check is performed to determine whether it can be captured\nmultiple times (which it can) and again the position of the group reset."),(0,a.kt)("p",null,"Finally we are on the last iteration through the FieldToken with the last remaining token being read which is a\nGrammarValue('field'). In this case though we have no clear closing syntax as fields are not followed by a '.'. This\nis where the Lexer has to use a bit of deductive reasoning which differs depending on the scenario. In our case we\nhave a single value and the end of the expression was met. As such, it is a safe bet to add it to the top stack\nitem. If we had the following scenario however:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"object.collection[0].field * 40\n")),(0,a.kt)("p",null,"The Lexer would then look at the top item in the stack and the following tokens to make a best guess. In this situation\nthe FieldToken does support a single TokenValue and is a valid match against the tokens pattern. Since that has a higher\nprobability of a match rather than existing as a single TokenValue between the statement and an Operator / Integer token,\nagain it is safe to assume that it can be added to that item."),(0,a.kt)("p",null,"This covers the process by which the Lexer matches tokens against statements using pattern tokens and how it deals with\nthe different types of grammar flags. As mentioned previously, this information is not strictly required for writing your\nown statements, but it is useful to understand why tokens are organised into the groups they are and the mechanism used\nto facilitate this."),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"NOTE"),": Although not covered here, when a grammar group is marked to capture multiple sets of tokens (as above), if\nit is found that the group is in its reset state (complete) but a following token after that group triggers a match,\nthe token position will be moved to that location to avoid being stuck in an endless cycle of repetitions. The lexer\nuses intelligent look ahead in this scenario."))}k.isMDXComponent=!0}}]);